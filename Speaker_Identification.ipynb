{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYTUBxnxqkgZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Audio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6PiRLW0y4RTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/My Drive/16000_pcm_speeches.zip\""
      ],
      "metadata": {
        "id": "qYvwS33n4m5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"/content/drive/My Drive/16000_pcm_speeches/Benjamin_Netanyau\"\n",
        "\n",
        "!cp -avr \"/content/drive/My Drive/16000_pcm_speeches\""
      ],
      "metadata": {
        "id": "6NVkxDCu47y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAJul6GMqkgc"
      },
      "outputs": [],
      "source": [
        "DATASET_ROOT = \"/content/drive/My Drive/16000_pcm_speeches\"\n",
        "\n",
        "AUDIO_SUBFOLDER = \"audio\"\n",
        "NOISE_SUBFOLDER = \"noise\"\n",
        "\n",
        "DATASET_AUDIO_PATH = os.path.join(DATASET_ROOT, AUDIO_SUBFOLDER)\n",
        "DATASET_NOISE_PATH = os.path.join(DATASET_ROOT, NOISE_SUBFOLDER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-tzyafLqkge"
      },
      "outputs": [],
      "source": [
        "VALID_SPLIT = 0.1\n",
        "\n",
        "SHUFFLE_SEED = 43\n",
        "\n",
        "SAMPLING_RATE = 16000\n",
        "\n",
        "SCALE = 0.5\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "EPOCHS = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU0zR33oqkge"
      },
      "source": [
        "Pre-processing DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4YMoJWMqkgh"
      },
      "outputs": [],
      "source": [
        "# If folder audio, does not exist, create it, otherwise do nothing\n",
        "if os.path.exists(DATASET_AUDIO_PATH) is False:\n",
        "    os.makedirs(DATASET_AUDIO_PATH)\n",
        "\n",
        "# If folder noise, does not exist, create it, otherwise do nothing\n",
        "if os.path.exists(DATASET_NOISE_PATH) is False:\n",
        "    os.makedirs(DATASET_NOISE_PATH)\n",
        "\n",
        "for folder in os.listdir(DATASET_ROOT):\n",
        "    if os.path.isdir(os.path.join(DATASET_ROOT, folder)):\n",
        "        if folder in [AUDIO_SUBFOLDER, NOISE_SUBFOLDER]:\n",
        "            # If folder is audio or noise, do nothing\n",
        "            continue\n",
        "        elif folder in [\"other\", \"_background_noise_\"]:\n",
        "            # If folder is one of the folders that contains noise samples move it to the noise folder\n",
        "            shutil.move(\n",
        "                os.path.join(DATASET_ROOT, folder),\n",
        "                os.path.join(DATASET_NOISE_PATH, folder),\n",
        "            )\n",
        "        else:\n",
        "            # Otherwise, it should be a speaker folder, then move it to audio folder\n",
        "            shutil.move(\n",
        "                os.path.join(DATASET_ROOT, folder),\n",
        "                os.path.join(DATASET_AUDIO_PATH, folder),\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg-m7zaCqkgi"
      },
      "source": [
        "Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fieYFo-uqkgj"
      },
      "outputs": [],
      "source": [
        "# Get the list of all noise files\n",
        "noise_paths = []\n",
        "for subdir in os.listdir(DATASET_NOISE_PATH):\n",
        "    subdir_path = Path(DATASET_NOISE_PATH) / subdir\n",
        "    if os.path.isdir(subdir_path):\n",
        "        noise_paths += [\n",
        "            os.path.join(subdir_path, filepath)\n",
        "            for filepath in os.listdir(subdir_path)\n",
        "            if filepath.endswith(\".wav\")\n",
        "        ]\n",
        "\n",
        "print(\"Found {} files belonging to {} directories\".format(len(noise_paths), len(os.listdir(DATASET_NOISE_PATH))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJj38xXoqkgk"
      },
      "outputs": [],
      "source": [
        "command = (\n",
        "    \"for dir in `ls -1 \" + DATASET_NOISE_PATH + \"`; do \"\n",
        "    \"for file in `ls -1 \" + DATASET_NOISE_PATH + \"/$dir/*.wav`; do \"\n",
        "    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n",
        "    \"$file | grep sample_rate | cut -f2 -d=`; \"\n",
        "    \"if [ $sample_rate -ne 16000 ]; then \"\n",
        "    \"ffmpeg -hide_banner -loglevel panic -y \"\n",
        "    \"-i $file -ar 16000 temp.wav; \"\n",
        "    \"mv temp.wav $file; \"\n",
        "    \"fi; done; done\"\n",
        ")\n",
        "os.system(command)\n",
        "\n",
        "# Split noise into chunks of 16,000 steps each\n",
        "'''\n",
        "def load_noise_sample(path):\n",
        "    sample, sampling_rate = tf.audio.decode_wav(\n",
        "        tf.io.read_file(path), desired_channels=1\n",
        "    )\n",
        "    if sampling_rate == SAMPLING_RATE:\n",
        "        # Number of slices of 16000 each that can be generated from the noise sample\n",
        "        slices = int(sample.shape[0] / SAMPLING_RATE)\n",
        "        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n",
        "        return sample\n",
        "    else:\n",
        "        print(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))\n",
        "        return None\n",
        "'''\n",
        "\n",
        "# Split noise into chunks of 16,000 steps each\n",
        "def load_noise_sample(path):\n",
        "    sample, sampling_rate = tf.audio.decode_wav(\n",
        "        tf.io.read_file(path), desired_channels=1\n",
        "    )\n",
        "    if sampling_rate == SAMPLING_RATE:\n",
        "        # Number of slices of 16000 each that can be generated from the noise sample\n",
        "        slices = int(sample.shape[0] / SAMPLING_RATE)\n",
        "        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n",
        "        return sample\n",
        "    else:\n",
        "        print(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))\n",
        "        return None\n",
        "\n",
        "\n",
        "noises = []\n",
        "for path in noise_paths:\n",
        "    sample = load_noise_sample(path)\n",
        "    if sample:\n",
        "        noises.extend(sample)\n",
        "\n",
        "if noises:  # Check if noises is not empty\n",
        "    noises = tf.stack(noises)\n",
        "\n",
        "    print(\n",
        "        \"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n",
        "            len(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE\n",
        "        )\n",
        "    )\n",
        "else:\n",
        "    print(\"No valid noise samples were found. Please check the sample rates of your files.\")\n",
        "\n",
        "\n",
        "noises = []\n",
        "for path in noise_paths:\n",
        "    sample = load_noise_sample(path)\n",
        "    if sample:\n",
        "        noises.extend(sample)\n",
        "noises = tf.stack(noises)\n",
        "\n",
        "print(\n",
        "    \"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n",
        "        len(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split noise into chunks of 16,000 steps each\n",
        "def load_noise_sample(path):\n",
        "    sample, sampling_rate = tf.audio.decode_wav(\n",
        "        tf.io.read_file(path), desired_channels=1\n",
        "    )\n",
        "\n",
        "    print(f\"File: {path} - Sample Rate: {sampling_rate}\")  # Debugging line\n",
        "\n",
        "    if sampling_rate == SAMPLING_RATE:\n",
        "        # Number of slices of 16000 each that can be generated from the noise sample\n",
        "        slices = int(sample.shape[0] / SAMPLING_RATE)\n",
        "        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n",
        "        return sample\n",
        "    else:\n",
        "        print(f\"Sampling rate for {path} is incorrect. Ignoring it\")  # Debugging line\n",
        "        return None\n",
        "\n",
        "\n",
        "noises = []\n",
        "for path in noise_paths:\n",
        "    print(f\"Processing file: {path}\")  # Debugging line\n",
        "    sample = load_noise_sample(path)\n",
        "    if sample:\n",
        "        noises.extend(sample)\n",
        "\n",
        "if noises:  # Check if noises is not empty\n",
        "    noises = tf.stack(noises)\n",
        "\n",
        "    print(\n",
        "        \"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n",
        "            len(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE\n",
        "        )\n",
        "    )\n",
        "else:\n",
        "    print(\"No valid noise samples were found. Please check the sample rates of your files.\")\n"
      ],
      "metadata": {
        "id": "lbN8N0c075-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def paths_and_labels_to_dataset(audio_paths, labels):\n",
        "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
        "\n",
        "\n",
        "def path_to_audio(path):\n",
        "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
        "    return tf.squeeze(audio, axis=-1)  # Ensure it's 1D\n",
        "\n",
        "\n",
        "\n",
        "def add_noise(audio, noises=None, scale=0.5):\n",
        "    if noises is not None:\n",
        "        # Create a random tensor of the same size as audio ranging from\n",
        "        # 0 to the number of noise stream samples that we have.\n",
        "        tf_rnd = tf.random.uniform(\n",
        "            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n",
        "        )\n",
        "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
        "\n",
        "        # Get the amplitude proportion between the audio and the noise\n",
        "        prop = tf.math.reduce_max(tf.expand_dims(audio, axis=-1), axis=1) / \\\n",
        "        tf.math.reduce_max(tf.expand_dims(noise, axis=-1), axis=1)\n",
        "\n",
        "\n",
        "        # Adding the rescaled noise to audio\n",
        "        audio = audio + noise * prop * scale\n",
        "\n",
        "    return audio\n",
        "\n",
        "\n",
        "def audio_to_fft(audio):\n",
        "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
        "    # we need to squeeze the dimensions and then expand them again\n",
        "    # after FFT\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    fft = tf.signal.fft(\n",
        "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
        "    )\n",
        "    fft = tf.expand_dims(fft, axis=-1)\n",
        "\n",
        "    # Return the absolute value of the first half of the FFT\n",
        "    # which represents the positive frequencies\n",
        "    return tf.math.abs(fft[:, : (audio.shape[1]), :])\n",
        "\n",
        "\n",
        "# Get the list of audio file paths along with their corresponding labels\n",
        "\n",
        "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
        "print(\"Our class names: {}\".format(class_names,))\n",
        "\n",
        "audio_paths = []\n",
        "labels = []\n",
        "for label, name in enumerate(class_names):\n",
        "    print(\"Processing speaker {}\".format(name,))\n",
        "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
        "    speaker_sample_paths = [\n",
        "        os.path.join(dir_path, filepath)\n",
        "        for filepath in os.listdir(dir_path)\n",
        "        if filepath.endswith(\".wav\")\n",
        "    ]\n",
        "    audio_paths += speaker_sample_paths\n",
        "    labels += [label] * len(speaker_sample_paths)\n",
        "\n",
        "print(\n",
        "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
        ")\n",
        "\n",
        "# Shuffle\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(audio_paths)\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Split into training and validation\n",
        "num_val_samples = int(VALID_SPLIT * len(audio_paths))\n",
        "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
        "train_audio_paths = audio_paths[:-num_val_samples]\n",
        "train_labels = labels[:-num_val_samples]\n",
        "\n",
        "print(\"Using {} files for validation.\".format(num_val_samples))\n",
        "valid_audio_paths = audio_paths[-num_val_samples:]\n",
        "valid_labels = labels[-num_val_samples:]\n",
        "\n",
        "# Create 2 datasets, one for training and the other for validation\n",
        "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
        "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "    BATCH_SIZE\n",
        ")\n",
        "\n",
        "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
        "\n",
        "\n",
        "# Add noise to the training set\n",
        "# train_ds = train_ds.map(\n",
        "#     lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n",
        "#     num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "# )\n",
        "\n",
        "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
        "# train_ds = train_ds.map(\n",
        "#     lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "train_ds = train_ds.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))\n",
        "print(\"--------\")\n",
        "print(train_ds)\n",
        "\n",
        "# valid_ds = valid_ds.map(\n",
        "#     lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "# )\n",
        "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "valid_ds = valid_ds.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))"
      ],
      "metadata": {
        "id": "RDblPIOO9SrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def load_dataset(data_path):\n",
        "    X = []\n",
        "    y = []\n",
        "    for speaker_dir in os.listdir(data_path):\n",
        "        speaker_path = os.path.join(data_path, speaker_dir)\n",
        "        if os.path.isdir(speaker_path) and speaker_dir != \"background_noise\":\n",
        "            for file in os.listdir(speaker_path):\n",
        "                file_path = os.path.join(speaker_path, file)\n",
        "                if file_path.endswith('.wav'):\n",
        "                    try:\n",
        "                        audio, sr = librosa.load(file_path, sr=16000)\n",
        "                        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "                        mfccs_mean = np.mean(mfccs.T, axis=0)\n",
        "                        X.append(mfccs_mean)\n",
        "                        y.append(speaker_dir)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {file_path}: {e}\")\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def preprocess_data(X, y):\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "    y_categorical = to_categorical(y_encoded)\n",
        "    return X, y_categorical, label_encoder\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=input_shape),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_model(X_train, y_train, X_test, y_test, model):\n",
        "    model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "def predict_speaker(audio_path, model, label_encoder):\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
        "    mfccs_mean = mfccs_mean.reshape(1, -1)\n",
        "    predictions = model.predict(mfccs_mean)\n",
        "    predicted_class = label_encoder.inverse_transform([np.argmax(predictions)])[0]\n",
        "    confidence = np.max(predictions)\n",
        "    return predicted_class, confidence\n",
        "\n",
        "\n",
        "\n",
        "DATASET_PATH = \"DATASET_AUDIO_PATH\"\n",
        "print(\"Loading dataset...\")\n",
        "X, y = load_dataset(DATASET_PATH)\n",
        "print(f\"Dataset loaded with {len(X)} samples.\")\n",
        "\n",
        "X, y, label_encoder = preprocess_data(X, y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = build_model(input_shape=(X_train.shape[1],), num_classes=y_train.shape[1])\n",
        "print(\"Training the model...\")\n",
        "model = train_model(X_train, y_train, X_test, y_test, model)\n",
        "\n",
        "print(\"Evaluating the model...\")\n",
        "accuracy = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "model.save(\"/kaggle/working/speaker_identification_model.h5\")\n",
        "np.save(\"/kaggle/working/label_encoder_classes.npy\", label_encoder.classes_)\n",
        "print(\"Model and label encoder saved.\")\n",
        "\n",
        "\n",
        "TEST_AUDIO_PATH = \"/kaggle/input/speaker-recognition-dataset/16000_pcm_speeches/Magaret_Tarcher/545.wav\"\n",
        "predicted_speaker, confidence = predict_speaker(TEST_AUDIO_PATH, model, label_encoder)\n",
        "if confidence:\n",
        "    print(f\"Predicted Speaker: {predicted_speaker}\")\n",
        "    print(f\"Confidence: {confidence * 100:.2f}%\")\n",
        "else:\n",
        "    print(f\"Error: {predicted_speaker}\")\n",
        "\n",
        "def evaluate_on_test_set(X_test, y_test, label_encoder):\n",
        "    print(\"Evaluating on the test set...\")\n",
        "    predictions = model.predict(X_test)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    true_labels = np.argmax(y_test, axis=1)\n",
        "    predicted_speakers = label_encoder.inverse_transform(predicted_labels)\n",
        "    true_speakers = label_encoder.inverse_transform(true_labels)\n",
        "    accuracy = np.mean(predicted_labels == true_labels)\n",
        "    print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for i in range(5):\n",
        "        print(f\"True Speaker: {true_speakers[i]} | Predicted Speaker: {predicted_speakers[i]}\")\n",
        "\n",
        "    return accuracy\n",
        "test_accuracy = evaluate_on_test_set(X_test, y_test, label_encoder)"
      ],
      "metadata": {
        "id": "x4ukGq59BRA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywxj9ieNqkgl"
      },
      "source": [
        "Data-set Gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uroY1MXwqkgm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def paths_and_labels_to_dataset(audio_paths, labels):\n",
        "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
        "\n",
        "\n",
        "def path_to_audio(path):\n",
        "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
        "    return audio\n",
        "\n",
        "'''\n",
        "def add_noise(audio, noises=None, scale=0.5):\n",
        "    if noises is not None:\n",
        "        # Create a random tensor of the same size as audio ranging from\n",
        "        # 0 to the number of noise stream samples that we have.\n",
        "        tf_rnd = tf.random.uniform(\n",
        "            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n",
        "        )\n",
        "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
        "\n",
        "        # Get the amplitude proportion between the audio and the noise\n",
        "        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n",
        "        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n",
        "\n",
        "        # Adding the rescaled noise to audio\n",
        "        audio = audio + noise * prop * scale\n",
        "\n",
        "    return audio\n",
        "\n",
        "'''\n",
        "def audio_to_fft(audio):\n",
        "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
        "    # we need to squeeze the dimensions and then expand them again\n",
        "    # after FFT\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    fft = tf.signal.fft(\n",
        "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
        "    )\n",
        "    fft = tf.expand_dims(fft, axis=-1)\n",
        "\n",
        "    # Return the absolute value of the first half of the FFT\n",
        "    # which represents the positive frequencies\n",
        "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
        "\n",
        "\n",
        "# Get the list of audio file paths along with their corresponding labels\n",
        "\n",
        "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
        "print(\"Our class names: {}\".format(class_names,))\n",
        "\n",
        "audio_paths = []\n",
        "labels = []\n",
        "for label, name in enumerate(class_names):\n",
        "    print(\"Processing speaker {}\".format(name,))\n",
        "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
        "    speaker_sample_paths = [\n",
        "        os.path.join(dir_path, filepath)\n",
        "        for filepath in os.listdir(dir_path)\n",
        "        if filepath.endswith(\".wav\")\n",
        "    ]\n",
        "    audio_paths += speaker_sample_paths\n",
        "    labels += [label] * len(speaker_sample_paths)\n",
        "\n",
        "print(\n",
        "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
        ")\n",
        "\n",
        "# Shuffle\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(audio_paths)\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Split into training and validation\n",
        "num_val_samples = int(VALID_SPLIT * len(audio_paths))\n",
        "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
        "train_audio_paths = audio_paths[:-num_val_samples]\n",
        "train_labels = labels[:-num_val_samples]\n",
        "\n",
        "print(\"Using {} files for validation.\".format(num_val_samples))\n",
        "valid_audio_paths = audio_paths[-num_val_samples:]\n",
        "valid_labels = labels[-num_val_samples:]\n",
        "\n",
        "# Create 2 datasets, one for training and the other for validation\n",
        "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
        "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "    BATCH_SIZE\n",
        ")\n",
        "\n",
        "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
        "\n",
        "'''\n",
        "# Add noise to the training set\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n",
        "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        ")'''\n",
        "\n",
        "noises = tf.stack(noises) if isinstance(noises, list) else noises\n",
        "\n",
        "# Add noise to the training set\n",
        "\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n",
        "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        ")\n",
        "\n",
        "def add_noise(audio, noises=None, scale=0.5):\n",
        "    if noises is not None:\n",
        "        noises = tf.convert_to_tensor(noises)  # Ensure noises is a Tensor\n",
        "\n",
        "        # ✅ Ensure audio is 2D: Convert [batch_size, length, 1] → [batch_size, length]\n",
        "        if len(audio.shape) == 3 and audio.shape[-1] == 1:\n",
        "            audio = tf.squeeze(audio, axis=-1)\n",
        "\n",
        "        # ✅ Ensure noises has correct shape\n",
        "        num_noises = tf.shape(noises)[0]\n",
        "        noise_shape = tf.shape(audio)[1]  # Length of the audio\n",
        "        noises = tf.ensure_shape(noises, [num_noises, noise_shape])\n",
        "\n",
        "        # ✅ Select random noise samples\n",
        "        tf_rnd = tf.random.uniform((tf.shape(audio)[0],), 0, num_noises, dtype=tf.int32)\n",
        "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
        "\n",
        "        # ✅ Compute noise scaling factor\n",
        "        audio_max = tf.math.reduce_max(audio, axis=1, keepdims=True)\n",
        "        noise_max = tf.math.reduce_max(noise, axis=1, keepdims=True)\n",
        "        prop = tf.where(noise_max > 0, audio_max / noise_max, tf.zeros_like(audio_max))\n",
        "\n",
        "        # ✅ Add scaled noise\n",
        "        audio = audio + noise * prop * scale\n",
        "\n",
        "        # ✅ Restore original shape [batch_size, length, 1]\n",
        "        audio = tf.expand_dims(audio, axis=-1)\n",
        "\n",
        "    return audio\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        ")\n",
        "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "valid_ds = valid_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        ")\n",
        "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7bafjN5qkgo"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0XcfXheqkgp"
      },
      "outputs": [],
      "source": [
        "def residual_block(x, filters, conv_num = 3, activation = \"relu\"):\n",
        "    s = keras.layers.Conv1D(filters, 1, padding = \"same\")(x)\n",
        "\n",
        "    for i in range(conv_num - 1):\n",
        "        x = keras.layers.Conv1D(filters, 3, padding = \"same\")(x)\n",
        "        x = keras.layers.Activation(activation)(x)\n",
        "\n",
        "    x = keras.layers.Conv1D(filters, 3, padding = \"same\")(x)\n",
        "    x = keras.layers.Add()([x, s])\n",
        "    x = keras.layers.Activation(activation)(x)\n",
        "\n",
        "    return keras.layers.MaxPool1D(pool_size = 2, strides = 2)(x)\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    inputs = keras.layers.Input(shape = input_shape, name = \"input\")\n",
        "\n",
        "    x = residual_block(inputs, 16, 2)\n",
        "    x = residual_block(inputs, 32, 2)\n",
        "    x = residual_block(inputs, 64, 3)\n",
        "    x = residual_block(inputs, 128, 3)\n",
        "    x = residual_block(inputs, 128, 3)\n",
        "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = keras.layers.Dense(num_classes, activation = \"softmax\", name = \"output\")(x)\n",
        "\n",
        "    return keras.models.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "model = build_model((SAMPLING_RATE // 2, 1), len(class_names))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_save_filename = \"model.h5\"\n",
        "\n",
        "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(model_save_filename, monitor=\"val_accuracy\", save_best_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjsIsF-bqkgq"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxPJUdpBqkgq"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_ds,\n",
        "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqtaOm7iqkgs"
      },
      "source": [
        "Saving the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3YBo9jdqkgs"
      },
      "outputs": [],
      "source": [
        "# saving the standard h5 model\n",
        "model.save('model.h5')\n",
        "\n",
        "# saving if the user want to use for edge devices using tflite\n",
        "tf.saved_model.save(model, \"model_keras_tflite\")\n",
        "# zipping the folder\n",
        "!zip -r model_keras_tflite.zip model_keras_tflite/\n",
        "# removing the folder\n",
        "!rm -rf model_keras_tflite/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRLV8ubRqkgt"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_FuHAHVqkgt"
      },
      "outputs": [],
      "source": [
        "print(model.evaluate(valid_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R47GPbDuqkgu"
      },
      "source": [
        "Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMd0FD4Aqkgu"
      },
      "outputs": [],
      "source": [
        "SAMPLES_TO_DISPLAY = 10\n",
        "\n",
        "test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "    BATCH_SIZE\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))\n",
        "\n",
        "for audios, labels in test_ds.take(1):\n",
        "    # Get the signal FFT\n",
        "    ffts = audio_to_fft(audios)\n",
        "    # Predict\n",
        "    y_pred = model.predict(ffts)\n",
        "    # Take random samples\n",
        "    rnd = np.random.randint(0, BATCH_SIZE, SAMPLES_TO_DISPLAY)\n",
        "    audios = audios.numpy()[rnd, :, :]\n",
        "    labels = labels.numpy()[rnd]\n",
        "    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
        "\n",
        "    for index in range(SAMPLES_TO_DISPLAY):\n",
        "        # For every sample, print the true and predicted label\n",
        "        # as well as run the voice with the noise\n",
        "        print(\n",
        "            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
        "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
        "                class_names[labels[index]],\n",
        "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
        "                class_names[y_pred[index]],\n",
        "            )\n",
        "        )\n",
        "        if labels[index] ==y_pred[index]:\n",
        "            print(\"Welcome\")\n",
        "        else:\n",
        "            print(\"Sorry\")\n",
        "        print(\"The speaker is\" if labels[index] == y_pred[index] else \"\", class_names[y_pred[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UWcjAFqqkgv"
      },
      "outputs": [],
      "source": [
        "#Predcit the speaker from the test dataset for real time pred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGIEbQmYqkgw"
      },
      "outputs": [],
      "source": [
        "def paths_to_dataset(audio_paths):\n",
        "\t\"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
        "\tpath_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "\t# audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
        "\treturn tf.data.Dataset.zip((path_ds))\n",
        "\n",
        "def predict(path, labels):\n",
        "\ttest = paths_and_labels_to_dataset(path, labels)\n",
        "\n",
        "\n",
        "\ttest = test.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "\tBATCH_SIZE\n",
        "\t)\n",
        "\ttest = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "\ttest = test.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))\n",
        "\n",
        "\tfor audios, labels in test.take(1):\n",
        "\t\t# Get the signal FFT\n",
        "\t\tffts = audio_to_fft(audios)\n",
        "\t\t# Predict\n",
        "\t\ty_pred = model.predict(ffts)\n",
        "\t\t# Take random samples\n",
        "\t\trnd = np.random.randint(0, 1, 1)\n",
        "\t\taudios = audios.numpy()[rnd, :]\n",
        "\t\tlabels = labels.numpy()[rnd]\n",
        "\t\ty_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
        "\n",
        "\t\tfor index in range(1):\n",
        "\t\t\t# For every sample, print the true and predicted label\n",
        "\t\t\t# as well as run the voice with the noise\n",
        "\t\t\tprint(\n",
        "\t\t\t\t\"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
        "\t\t\t\t\t\"[92m\",y_pred[index],\n",
        "\t\t\t\t\t\"[92m\", y_pred[index]\n",
        "\t\t\t\t)\n",
        "\t\t\t)\n",
        "\t\t\tif class_names[y_pred[index]] == \"Julia_Gillard\":\n",
        "\t\t\t\tprint(\"Welcome\")\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"Sorry\")\n",
        "\t\t\tprint(class_names[y_pred[index]])\n",
        "\t\t\t# display(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))\n",
        "\n",
        "# predict(\"content/1000.wav\")\n",
        "\n",
        "path = [\"/content/0.wav\"]\n",
        "labels = [\"unknown\"]\n",
        "\n",
        "# path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "# audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
        "# label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "# return tf.data.Dataset.zip((audio_ds, label_ds))\n",
        "\n",
        "try:\n",
        "    predict(path, labels)\n",
        "except:\n",
        "    print(\"Error! Check if the file correctly passed or not!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}